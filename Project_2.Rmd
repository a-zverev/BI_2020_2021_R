---
title: 'Project_2: Order of Landlords'
author: "Aleksei Zverev"
date: "11/28/2020"
output: 
  html_document: 
    keep_md: yes
    number_section: yes
    toc: yes
    toc_float: yes
---

# Intro

```{r echo = FALSE, message = FALSE,	warning = FALSE}

library(MASS)
library(tidyverse)
library(corrplot)
library(lattice)
library(car)
library(ggpubr)

data(Boston)
data <- Boston
```

This data frame contains the following columns:

* `crim`- per capita crime rate by town.
* `zn`- proportion of residential land zoned for lots over 25,000 sq.ft.
* `indus` - proportion of non-retail business acres per town.
* `chas` - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* `nox` - nitrogen oxides concentration (parts per 10 million).
* `rm` - average number of rooms per dwelling.
* `age` - proportion of owner-occupied units built prior to 1940.
* `dis` - weighted mean of distances to five Boston employment centres.
* `rad` - index of accessibility to radial highways.
* `tax` - full-value property-tax rate per \$10,000.
* `ptratio` - pupil-teacher ratio by town.
* `black` - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
* `lstat` - lower status of the population (percent).
* `medv` - median value of owner-occupied homes in \$1000s.

# Explore variables

Here we see at our data metrics and try to understand, what they are
```{r}
str(data) # structure of dataset

sum(is.na(data)) # are there any NAs?
```

Explore correlation in our data:

```{r}
corrplot(cor(data), method = "number", type = "lower", tl.col = 'black', number.cex=0.5)
```



# Main part: linear model

So, we definetely know that `chas` is factor variable - let use it in a model as a factor. Also we have `rad` and there situation is more complex: if we transform it into factor, we will have this levels:

```{r}
levels(as.factor(data$rad))
```
24 looks like outlier, but the distribution is quite strange: there is high amount of data with rad = 24.
 
```{r}
histogram(Boston$rad)
```
 
I think, now we will save it as a plain data, not as a factor, and will work with it in an Additional part

## Standartization of predictors, full linear model

Here we standartise all our variables (except that, what we believe is a factor and our data for prediction), calculate linear model for all predictors and see at summary of a model

```{r warning=FALSE}
norm.data <- data %>% mutate_each_(list(~scale(.) %>% as.vector), vars=vars(-c("medv", "chas")))
norm.data$chas <- as.factor(norm.data$chas)

standartised.model <- lm(medv ~ ., norm.data)
summary(standartised.model)
```

Ok, two variables, `indus` and `age` are not significant. Remove them (and fortify data in this step)

```{r}
standartised.model.2 <- update(standartised.model, .~. - indus - age)
summary(standartised.model.2)

model <- fortify(standartised.model.2)
```

Ok, here is our model. Adjusted R-squared is 0,734 and p-value is less of threshold, so we can start to analyse it

## Diagnostic of linear model

### Linearity check

```{r}
ggplot(data = model, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
```

Our correspondense is close, but not linear. Perhaps, we should use better model :/

### Significant values (Cook's distance)

```{r}
ggplot(model, aes(x = 1:nrow(model), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```

All is good. No heavy observations.

## Independence of predictors

```{r message=FALSE, warning=FALSE}

predictors.from.model <- c("crim","zn","chas","nox","rm","dis","rad","tax","ptratio","black","lstat")

draw_residual <- function(column){
  ggplot(data = model, aes(x = data[,column], y = .stdresid)) + 
  geom_point() +  geom_smooth(method = "lm") + labs(x = column)
}

lapply(predictors.from.model, draw_residual)

```

Everything looks good

## Normality check and dispersion of residuals

```{r message=FALSE, warning=FALSE}
qqPlot(model$.fitted)

ggplot(data = model, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)
```

Fine - expected values are normal. But dispersion distribution have troubles.

## Draw expected price

maximum influence is in `lstat` variable, so, draw a graph of predictted price from percent lower status of the population

```{r}
expect.dataset <- norm.data
expect.dataset <- expect.dataset %>% mutate_at(.vars=c("crim","zn","nox","rm","dis","rad","tax","ptratio","black"), .funs = mean) %>% mutate(chas = as.factor(0))

Predictions <- predict(standartised.model.2, newdata = expect.dataset,  interval = 'confidence')
expect.dataset <- data.frame(expect.dataset, Predictions)


ggplot(expect.dataset, aes(x = lstat, y = fit)) +
  geom_ribbon(alpha = 0.2, aes(ymin = lwr, ymax = upr)) +
  geom_line() + 
  ggtitle("Multiple linear model: M(lstat)")
```

# Additional part: free explore

Here we will try to find optimal model for our data. 

## Explore and modify data

First of all, explore our data again:

```{r}
full.predictors <- c("crim","zn","chas","nox","rm","dis","rad","tax","ptratio","black","lstat", "indus", "age")

draw_correlation <- function(column){
  ggplot(data = data, aes(x = data[,column], y = medv)) + 
  geom_point() + labs(x = column)
}

lapply(full.predictors, draw_correlation)
```

What we should do:

* take log from `crime`
* take 1/x from `dis`
* group `rad` to two-level factor
* `black` looks strange - why it is so complex? Squared variable from average in formula makes me cry: is it a black hood, or block of white collars - digits are same. It also do not show more, than a traces of correlation - I think, in this form it's sensless data, and I'll remove it entirely

```{r}
data$crim.log <- log(data$crim)
draw_correlation('crim.log')

data$dis.rev <- 1/data$dis
draw_correlation('dis.rev')

data$rad.lev <- data$rad
data$rad.lev[data$rad.lev > 20] <- 24
data$rad.lev[data$rad.lev < 20] <- 4
data$rad.lev <- as.factor(data$rad.lev)
draw_correlation('rad.lev')

data$chas <- as.factor(data$chas)
```

So, our predictors are: `crim.log`,`zn`,`chas`(factor),`nox`,`rm`,`dis.rev`,`rad.lev`(factor),`tax`,`ptratio`, `lstat`, `indus` and `age`


## Create a model

```{r}
free.model <- lm(medv ~ crim.log + zn + chas + nox + rm + dis.rev + rad.lev + tax + ptratio + lstat + indus + age, data)
summary(free.model)
```

Compare with same for normalised data

```{r}
n.data <- data %>% mutate_each_(list(~scale(.) %>% as.vector), vars=vars(-c("medv", "chas", "rad.lev")))
free.nmodel <- lm(medv ~ crim.log + zn + chas + nox + rm + dis.rev + rad.lev + tax + ptratio + lstat + indus + age, n.data)
summary(free.nmodel)
```

All the same, so, in a name of Ockham, we will use non-normalised data. Remove non-significant:

```{r}
free.model <- lm(medv ~ chas + nox + rm + dis.rev + tax + ptratio + lstat + rad.lev + chas, data)
summary(free.model)
```

Adjusted R is better after removing of predictors, so, we do right things!

## Multicollinear check

We will drop one-by one most collinear predictors and see ar R-squared of model - if it drops, it would be a sign no think twice about predictor

```{r}
vif(free.model)
free.model <- update(free.model, .~. - rad.lev)
summary(free.model)$adj.r.squared

vif(free.model)
free.model <- update(free.model, .~. - nox)
summary(free.model)$adj.r.squared

vif(free.model)
free.model <- update(free.model, .~. - lstat)
summary(free.model)$adj.r.squared
```

Oops - `lstat` we should save. Try another one:

```{r}
free.model <- update(free.model, .~. + lstat)
summary(free.model)$adj.r.squared

free.model <- update(free.model, .~. - tax)
summary(free.model)$adj.r.squared

vif(free.model)
```

Ok, `lstat` is significant for us and not critical by VIF, so, our potential model is

```{r}
summary(free.model)
free.model.f <- fortify(free.model)
```

## Diagnostic of free linear model

### Linearity check

```{r}
ggplot(data = free.model.f, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
```

Same pattern - our best model isn`t better in this case :/

### Significant values (Cook's distance)

```{r}
ggplot(free.model.f, aes(x = 1:nrow(free.model.f), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```

All is good.

## Independence of predictors

```{r message=FALSE, warning=FALSE}

predictors.from.model <- c("chas", "rm", "dis.rev", "ptratio", "lstat")

draw_residual <- function(column){
  ggplot(data = free.model.f, aes(x = free.model.f[,column], y = .stdresid)) + 
  geom_point() +  geom_smooth(method = "lm") + labs(x = column)
}

lapply(predictors.from.model, draw_residual)

```

Fine

## Normality check and dispersion of residuals

```{r message=FALSE, warning=FALSE}
qqPlot(free.model.f$.fitted)

ggplot(data = free.model.f, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)
```

Expected values are normal. But dispersion distribution still have troubles.

# Short run along the price

What exactly are our prices?

```{r}
histogram(data$medv)
```

Perhaps, we should split our dataset to common and expensive houses?

```{r}
data.common <- data %>% filter(medv <= 40)
data.expensive <- data %>% filter(medv > 40)

histogram(data.common$medv)
nrow(data.common)

histogram(data.expensive$medv)
nrow(data.expensive)
```

Too few data for expensive houses. Let's focuse on common. Rebuild our common model


## Create a model

```{r}
free.common.model <- lm(medv ~ crim.log + zn + chas + nox + rm + dis.rev + rad.lev + tax + ptratio + lstat + indus + age, data.common)
summary(free.common.model)
```

Remove non-significant:

```{r}
free.common.model <- lm(medv ~ nox + rm + dis.rev + tax + ptratio + lstat + rad.lev, data.common)
summary(free.common.model)
```

## Multicollinear check


```{r}
vif(free.common.model)
free.common.model <- update(free.common.model, .~. - rad.lev)
summary(free.common.model)$adj.r.squared

vif(free.common.model)
free.common.model <- update(free.common.model, .~. - dis.rev)
summary(free.common.model)$adj.r.squared

vif(free.common.model)
free.common.model <- update(free.common.model, .~. - nox)
summary(free.common.model)$adj.r.squared

vif(free.common.model)
```

Our potential model for common buildings is

```{r}
summary(free.common.model)
free.common.model.f <- fortify(free.common.model)
```

### Linearity check

```{r}
ggplot(data = free.common.model.f, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
```

Actually, the same troubles :(

### Significant values (Cook's distance)

```{r}
ggplot(free.common.model.f, aes(x = 1:nrow(free.common.model.f), y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red")
```

All is good.

## Independence of predictors

```{r message=FALSE, warning=FALSE}

predictors.from.model <- c("tax", "rm", "ptratio", "lstat")

draw_residual <- function(column){
  ggplot(data = free.common.model.f, aes(x = free.common.model.f[,column], y = .stdresid)) + 
  geom_point() +  geom_smooth(method = "lm") + labs(x = column)
}

lapply(predictors.from.model, draw_residual)

```


## Normality check and dispersion of residuals

```{r message=FALSE, warning=FALSE}
qqPlot(free.common.model.f$.fitted)

ggplot(data = free.common.model.f, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)
```

Still not good


# Recommendation for building

So, our models are:

```{r}
summary(free.model) # "chas", "rm", "dis.rev", "ptratio", "lstat"
summary(free.common.model) # "tax", "rm", "ptratio", "lstat"
```

Actualy, both models have `rm`, `ptratio` and `lstat`. Both have same adj-Rsq, and both, as we saw, fail dispersion distribution test. So, both of them isn't really good. But, if we believe in them...

## Building recomendation

If you want to build just a house, there is recomendations:

* build house near the river - it's better (`chas`)
* make more rooms per person (so unexpected) (`rm`)
* build it near employment center(`dis.rev`)
* find a place with maximum amount of teachers per pupil (`ptratio`)
* find place with lowest percent of lower-status sitizens (`lstat`)

## Common building recommendation

If you want to build a regular (not expensive) house, there is another cases:

* build in a location with lower property-tax rate (`tax`)
* make more rooms per person (`rm`)
* find a place with maximum amount of teachers per pupil (`ptratio`)
* find place with lowest percent of lower-status sitizens (`lstat`)

